prepped_pen_recipe <- prep(log_recipe)
bake(prepped_pen_recipe, new_data = log_bike)
# Tuning Models -----------------------------------------------------------
lin_pen_model <- linear_reg(penalty=tune(),
mixture=tune()) %>%
set_engine('glmnet')
## Set Workflow
lin_pen_wf <- workflow() %>%
add_recipe(log_recipe) %>%
add_model(pois_pen_model)
## Set Workflow
lin_pen_wf <- workflow() %>%
add_recipe(log_recipe) %>%
add_model(lin_pen_model)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
tuning_grid
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 2)
tuning_grid
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- lin_pen_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Plot Results
collet_metrics(CV_results) %>% #Gathers metrics into DF
filter(.metric=='rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Plot Results
collect_metrics(CV_results) %>% #Gathers metrics into DF
filter(.metric=='rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
best_Tune <- CV_results %>%
select_best('rmse')
## Finalize the Workflow & fit it
final_wf <- lin_pen_wf %>%
finalize_workflow(bestTune) %>%
fit(data = log_bike)
## Find Best Tuning Parameters
best_Tune <- CV_results %>%
select_best('rmse')
## Finalize the Workflow & fit it
final_wf <- lin_pen_wf %>%
finalize_workflow(bestTune) %>%
fit(data = log_bike)
## Finalize the Workflow & fit it
final_wf <- lin_pen_wf %>%
finalize_workflow(best_Tune) %>%
fit(data = log_bike)
## Predict
final_wf %>%
predict(new_data = test)
## Predict
final_wf %>%
predict(new_data = test)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_bike
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# Tuning Models -----------------------------------------------------------
lin_pen_model <- linear_reg(penalty=tune(),
mixture=tune()) %>%
set_engine('glmnet')
## Set Workflow
lin_pen_wf <- workflow() %>%
add_recipe(log_recipe) %>%
add_model(lin_pen_model)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- lin_pen_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Plot Results
collect_metrics(CV_results) %>% #Gathers metrics into DF
filter(.metric=='rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
best_Tune <- CV_results %>%
select_best('rmse')
## Finalize the Workflow & fit it
final_wf <- lin_pen_wf %>%
finalize_workflow(best_Tune) %>%
fit(data = log_bike)
## Predict
final_wf %>%
predict(new_data = test)
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_nzv(all_numeric_predictors())
prepped_pen_recipe <- prep(log_recipe)
# Tuning Models -----------------------------------------------------------
lin_pen_model <- linear_reg(penalty=tune(),
mixture=tune()) %>%
set_engine('glmnet')
## Set Workflow
lin_pen_wf <- workflow() %>%
add_recipe(log_recipe) %>%
add_model(lin_pen_model)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- lin_pen_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Plot Results
collect_metrics(CV_results) %>% #Gathers metrics into DF
filter(.metric=='rmse') %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
best_Tune <- CV_results %>%
select_best('rmse')
## Finalize the Workflow & fit it
final_wf <- lin_pen_wf %>%
finalize_workflow(best_Tune) %>%
fit(data = log_bike)
## Predict
final_wf %>%
predict(new_data = test)
## Predict
lin_pen_preds <- final_wf %>%
predict(new_data = test)
## Predict
lin_pen_predictions <- final_wf %>%
predict(new_data = test)
colnames(lin_pen_preds) <- c('datetime', 'count')
# Change formatting of datetime
lin_pen_preds$datetime <- as.character(lin_pen_preds$datetime)
# Write that dataset to a csv file
vroom_write(lin_pen_preds, 'lin_pen_preds.csv', ",")
lin_pen_predictions
final_wf
lin_pen_predictions
# Create a dataframe that only has datetime and predictions (To upload to Kaggle)
lin_pen_preds <- data.frame(test$datetime, lin_pen_predictions)
lin_pen_preds
colnames(lin_pen_preds) <- c('datetime', 'count')
lin_pen_preds
# Change formatting of datetime
lin_pen_preds$datetime <- as.character(lin_pen_preds$datetime)
# Write that dataset to a csv file
vroom_write(lin_pen_preds, 'lin_pen_preds.csv', ",")
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
test <- vroom("./test.csv")
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
view(bike)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_nzv(all_numeric_predictors())
bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
bike %>%
bike %>%
select(-casual, -registered)
bike <- vroom("./train.csv")
bike %>%
select(-casual, -registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_nzv(all_numeric_predictors())
prepped_pen_recipe <- prep(log_recipe)
bake(prepped_pen_recipe, new_data = log_bike)
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_nzv(all_numeric_predictors())
prepped_log_recipe <- prep(log_recipe)
bake(prepped_pen_recipe, new_data = log_bike)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>% # Type of model
set_engine('rpart') %>%
set_mode('regression')
## Set Workflow
tree_wf <- workflow() %>%
add_recipe(log_recipe) %>%
add_model(tree_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_nzv(all_numeric_predictors())
prepped_log_recipe <- prep(log_recipe)
bake(prepped_log_recipe, new_data = log_bike)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>% # Type of model
set_engine('rpart') %>%
set_mode('regression')
## Set Workflow
tree_wf <- workflow() %>%
add_recipe(log_recipe) %>%
add_model(tree_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Grid of values to tune over
tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Split data for CV
folds <- vfold_cv(log_bike, v = 5, repeats=5)
## Run the CV
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Find Best Tuning Parameters
best_Tune <- CV_results %>%
select_best('rmse')
## Run the CV
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Run the CV
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
## Find Best Tuning Parameters
best_Tune <- CV_results %>%
select_best('rmse')
## Finalize the Workflow & fit it
final_tree_wf <- tree_wf %>%
finalize_workflow(best_Tune) %>%
fit(data = log_bike)
test <- vroom("./test.csv")
## Predict
## Get Predictions for test set AND format for Kaggle
tree_preds <- predict(tree_wf, new_data = test) %>% #This predicts log(count)
mutate(.pred=exp(.pred)) %>% # Back-transform the log to original scale
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Predict
## Get Predictions for test set AND format for Kaggle
tree_preds <- predict(final_tree_wf, new_data = test) %>% #This predicts log(count)
mutate(.pred=exp(.pred)) %>% # Back-transform the log to original scale
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write predictions to CSV
vroom_write(x=tree_preds, file="./tree_preds.csv", delim=",")
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
test <- vroom("./test.csv")
# Random Forests ----------------------------------------------------------
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
bike <- bike %>%
select(-casual, -registered)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
test <- vroom("./test.csv")
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
head(bike)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
bike <- vroom("./train.csv")
head(bike)
bike <- bike %>%
select(-casual, -registered)
head(bike)
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
head(bike)
test <- vroom("./test.csv")
# Random Forests ----------------------------------------------------------
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
log_bike
log_bike <- bike %>%
mutate(count=log(count)) %>%
select(-casual,-registered)
bike %>%
mutate(count=log(count)) %>%
select(-casual, -registered)
log_bike <- bike %>%
mutate(count=log(count))
head(log_bike)
log_recipe <- recipe(count ~ ., data = log_bike) %>%
step_time(datetime, features=c("hour")) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>%
step_num2factor(season, levels=c("spring", "summer", "fall", "winter")) %>%
step_num2factor(weather, levels=c("partly_cloudy", "misty", "rainy")) %>%
step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("no", "yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("no", "yes"))) %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_nzv(all_numeric_predictors())
install.packages('rpart')
install.packages("rpart")
install.packages('rpart')
library(tidymodels)
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
forest_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine('ranger') %>%  # What R function to use
set_model('regression')
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
forest_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine('ranger') %>%  # What R function to use
set_model('regression')
library(tidymodels)
forest_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine('ranger') %>%  # What R function to use
set_model('regression')
# Random Forests ----------------------------------------------------------
install.packages('rpart')
install.packages("rpart")
forest_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>%
set_engine('ranger') %>%  # What R function to use
set_model('regression')
